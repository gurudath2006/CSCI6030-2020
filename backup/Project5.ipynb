{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " clean_text: \n",
      "  INTRODUCTION \n",
      "In traditional cognitive science inquiry measurement almost \n",
      "always involves significa\n",
      "\n",
      " Few sentences are here: \n",
      " ['introduction in traditional cognitive science inquiry, measurement almost always involves significant levels of abstraction away from the phenomena of interest.', 'usually, observable behavior is of interest because it is assumed to indicate cognitive phenomena.', 'for example, in learning measurement, the factors of interest are unobservable, so behavior observation is used as a proxy.', 'in online learning environments, however, even greater abstraction is required in order to conduct inquiry.', 'behavior, in most online learning scenarios, is not directly observable.']\n",
      "\n",
      " Tokens: \n",
      "  ['INTRODUCTION', 'In', 'traditional', 'cognitive', 'science', 'inquiry', 'measurement', 'almost', 'always', 'involves', 'significant', 'levels', 'of', 'abstraction', 'away', 'from', 'the', 'phenomena', 'of', 'interest', 'Usually', 'observable', 'behavior', 'is', 'of', 'interest', 'because', 'it', 'is', 'assumed', 'to', 'indicate', 'cognitive', 'phenomena', 'For', 'example', 'in', 'learning', 'measurement', 'the', 'factors', 'of', 'interest', 'are', 'unobservable', 'so', 'behavior', 'observation', 'is', 'used']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "# load data\n",
    "filename = 'TestData_sample.txt'\n",
    "file = open(filename, encoding=\"iso-8859-1\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "def sentence(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = re.sub('\\d', '',text)\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \"List all the sentences in a text. Normalize to lowercase.\"\n",
    "    pattern = re.compile(r'([a-zA-Z][^\\.!?]*[\\.!?])', re.M)\n",
    "    #clean_text_pattern= \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "#clean punctuations\n",
    "def clean_text(text):\n",
    " #   text=text.lower()\n",
    " #   clean_text= re.split(r'\\W+',text.strip())\n",
    "    clean_text= \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return clean_text\n",
    "\n",
    "clean_text= clean_text(text)\n",
    "print(\"\\n clean_text: \\n \",clean_text[:100])\n",
    "\n",
    "sentences=sentence(text)\n",
    "print(\"\\n Few sentences are here: \\n\",sentences[:5])\n",
    "\n",
    "#split into words(tokenize)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(clean_text)\n",
    "print(\"\\n Tokens: \\n \",tokens[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Generate N Gram Models###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " unigrams: \n",
      " \n",
      " [['INTRODUCTION'], ['In'], ['traditional'], ['cognitive'], ['science'], ['inquiry'], ['measurement'], ['almost'], ['always'], ['involves'], ['significant'], ['levels'], ['of'], ['abstraction'], ['away'], ['from'], ['the'], ['phenomena'], ['of'], ['interest']]\n",
      "\n",
      " \n",
      " bigrams: \n",
      " \n",
      " [['INTRODUCTION', 'In'], ['In', 'traditional'], ['traditional', 'cognitive'], ['cognitive', 'science'], ['science', 'inquiry'], ['inquiry', 'measurement'], ['measurement', 'almost'], ['almost', 'always'], ['always', 'involves'], ['involves', 'significant'], ['significant', 'levels'], ['levels', 'of'], ['of', 'abstraction'], ['abstraction', 'away'], ['away', 'from'], ['from', 'the'], ['the', 'phenomena'], ['phenomena', 'of'], ['of', 'interest'], ['interest', 'Usually']]\n",
      "\n",
      " \n",
      " trigrams: \n",
      " \n",
      " [['INTRODUCTION', 'In', 'traditional'], ['In', 'traditional', 'cognitive'], ['traditional', 'cognitive', 'science'], ['cognitive', 'science', 'inquiry'], ['science', 'inquiry', 'measurement'], ['inquiry', 'measurement', 'almost'], ['measurement', 'almost', 'always'], ['almost', 'always', 'involves'], ['always', 'involves', 'significant'], ['involves', 'significant', 'levels'], ['significant', 'levels', 'of'], ['levels', 'of', 'abstraction'], ['of', 'abstraction', 'away'], ['abstraction', 'away', 'from'], ['away', 'from', 'the'], ['from', 'the', 'phenomena'], ['the', 'phenomena', 'of'], ['phenomena', 'of', 'interest'], ['of', 'interest', 'Usually'], ['interest', 'Usually', 'observable']]\n"
     ]
    }
   ],
   "source": [
    "#Ngrams model\n",
    "def getNGrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "unigrams= getNGrams(tokens,1)\n",
    "print(\"\\n \\n unigrams: \\n \\n\",unigrams[:20])\n",
    "\n",
    "bigrams = getNGrams(tokens,2)\n",
    "print(\"\\n \\n bigrams: \\n \\n\",bigrams[:20])\n",
    "\n",
    "trigrams= getNGrams(tokens,3)\n",
    "print(\"\\n \\n trigrams: \\n \\n\",trigrams[:20])\n",
    "\n",
    "\n",
    "nValue =int(input(\"Enter n value for N grams\"))\n",
    "ngrams = getNGrams(tokens,nValue)\n",
    "print(\"\\n \\n N grams for given input value \\n \\n\",ngrams[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ngrams model along with its frequency/counts for a given dataset\n",
    "from itertools import islice\n",
    "def ngrams_counts(tokens, n):\n",
    "    # create an empty dictionary\n",
    "    output = {}\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        # generate an ngram\n",
    "        g = ' '.join(tokens[i:i+n])\n",
    "        # set output[key] = 0, if key is not already in dictionary; otherwise, no effect\n",
    "        output.setdefault(g, 0)\n",
    "        # increment the ngram count\n",
    "        output[g] += 1\n",
    "    \n",
    "    # return the dictionary of ngrams (includes frequency of occurrence)\n",
    "    return output\n",
    "\n",
    "\n",
    "unigram_counts = ngrams_counts(tokens, 1)\n",
    "print(\" \\n \\n unigrams with frequency \\n \\n\",list(islice(unigram_counts.items(), 10)))\n",
    "\n",
    "bigram_counts = ngrams_counts(tokens, 2)\n",
    "print(\" \\n \\n bigrams with frequency \\n \\n\",list(islice(bigram_counts.items(), 10)))\n",
    "\n",
    "trigram_counts = ngrams_counts(tokens, 3)\n",
    "print(\" \\n \\n bigrams with frequency \\n \\n\",list(islice(trigram_counts.items(), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
