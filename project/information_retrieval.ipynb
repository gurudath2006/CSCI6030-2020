{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_for_text(dirPath, fileName):\n",
    "    file = os.path.join(dirName, filename)\n",
    "    file = open(file,'r', encoding=\"utf-8\")\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to clear punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuations(text):\n",
    "    clean_text = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to filter stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(text):\n",
    "    filteredTokens = []\n",
    "    from nltk.corpus import stopwords \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for r in tokens: \n",
    "        if not r in stop_words: \n",
    "            filteredTokens.append(r)\n",
    "    return filteredTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower_case(tokens):\n",
    "    tokens_lower = [w.lower() for w in tokens]\n",
    "    return tokens_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-(n-1)):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Ngrams model along with its frequency/counts for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ngrams model along with its frequency/counts for a given dataset\n",
    "from itertools import islice\n",
    "def ngrams_counts(tokens, n):\n",
    "    # create an empty dictionary\n",
    "    output = {}\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        # generate an ngram\n",
    "        g = ' '.join(tokens[i:i+n])\n",
    "        # set output[key] = 0, if key is not already in dictionary; otherwise, no effect\n",
    "        output.setdefault(g, 0)\n",
    "        # increment the ngram count\n",
    "        output[g] += 1\n",
    "    \n",
    "    # return the dictionary of ngrams (includes frequency of occurrence)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : \n",
      " \n",
      " ['abstract', 'in', 'westerman', '’', '12', 'disruptive', 'article', '“', 'quantitative', 'research', 'interpretive', 'enterprise', 'the', 'mostly', 'unacknowledged', 'role', 'interpretation', 'research', 'efforts', 'suggestions', 'explicitly', 'interpretive', 'quantitative', 'investigations', '”', 'invited', 'qualitative', 'researchers', 'psychology', 'adopt', 'quantitative', 'methods', 'interpretive', 'inquiry', 'given', 'capable', 'qualitative', 'measures', 'producing', 'meaningladen', 'results', 'the', 'objective', 'article', 'identify', 'westerman', '’', '12', 'key', 'arguments', 'apply', 'practice', 'learning', 'analytics', 'educational', 'interventions', 'the', 'primary', 'implication', 'learning', 'analytics', 'practitioners', 'need', 'interpret', 'quantitative', 'analysis', 'procedures', 'every', 'phase', 'philosophy', 'conclusions', 'furthermore', 'learning', 'analytics', 'practitioners', 'consumers', 'must', 'critically', 'examine', 'assumption', 'suggests', 'quantitative', 'methodologies', 'learning', 'analytics', 'inherently', 'objective', 'learning', 'analytics', 'algorithms', 'may', 'replace', 'judgment', 'rather', 'aid', 'lastly', 'propose', 'method', 'making', 'observational', 'data', 'virtual', 'environments', 'concrete', 'nested', 'models', 'in', 'traditional', 'cognitive', 'science', 'inquiry', 'measurement', 'almost', 'always', 'involves', 'significant', 'levels', 'abstraction', 'away', 'phenomena', 'interest', 'usually', 'observable', 'behavior', 'interest', 'assumed', 'indicate', 'cognitive', 'phenomena', 'for', 'example', 'learning', 'measurement', 'factors', 'interest']\n",
      "\n",
      " \n",
      " unigrams: \n",
      " \n",
      " [['abstract'], ['in'], ['westerman'], ['’'], ['12'], ['disruptive'], ['article'], ['“'], ['quantitative'], ['research'], ['interpretive'], ['enterprise'], ['the'], ['mostly'], ['unacknowledged'], ['role'], ['interpretation'], ['research'], ['efforts'], ['suggestions']]\n",
      "\n",
      " \n",
      " bigrams: \n",
      " \n",
      " [['abstract', 'in'], ['in', 'westerman'], ['westerman', '’'], ['’', '12'], ['12', 'disruptive'], ['disruptive', 'article'], ['article', '“'], ['“', 'quantitative'], ['quantitative', 'research'], ['research', 'interpretive'], ['interpretive', 'enterprise'], ['enterprise', 'the'], ['the', 'mostly'], ['mostly', 'unacknowledged'], ['unacknowledged', 'role'], ['role', 'interpretation'], ['interpretation', 'research'], ['research', 'efforts'], ['efforts', 'suggestions'], ['suggestions', 'explicitly']]\n",
      "\n",
      " \n",
      " trigrams: \n",
      " \n",
      " [['abstract', 'in', 'westerman'], ['in', 'westerman', '’'], ['westerman', '’', '12'], ['’', '12', 'disruptive'], ['12', 'disruptive', 'article'], ['disruptive', 'article', '“'], ['article', '“', 'quantitative'], ['“', 'quantitative', 'research'], ['quantitative', 'research', 'interpretive'], ['research', 'interpretive', 'enterprise'], ['interpretive', 'enterprise', 'the'], ['enterprise', 'the', 'mostly'], ['the', 'mostly', 'unacknowledged'], ['mostly', 'unacknowledged', 'role'], ['unacknowledged', 'role', 'interpretation'], ['role', 'interpretation', 'research'], ['interpretation', 'research', 'efforts'], ['research', 'efforts', 'suggestions'], ['efforts', 'suggestions', 'explicitly'], ['suggestions', 'explicitly', 'interpretive']]\n",
      " \n",
      " \n",
      " unigrams with frequency \n",
      " \n",
      " [('abstract', 1), ('in', 2), ('westerman', 2), ('’', 2), ('12', 2), ('disruptive', 1), ('article', 2), ('“', 1), ('quantitative', 5), ('research', 2)]\n",
      " \n",
      " \n",
      " bigrams with frequency \n",
      " \n",
      " [('abstract in', 1), ('in westerman', 1), ('westerman ’', 2), ('’ 12', 2), ('12 disruptive', 1), ('disruptive article', 1), ('article “', 1), ('“ quantitative', 1), ('quantitative research', 1), ('research interpretive', 1)]\n",
      " \n",
      " \n",
      " bigrams with frequency \n",
      " \n",
      " [('abstract in westerman', 1), ('in westerman ’', 1), ('westerman ’ 12', 2), ('’ 12 disruptive', 1), ('12 disruptive article', 1), ('disruptive article “', 1), ('article “ quantitative', 1), ('“ quantitative research', 1), ('quantitative research interpretive', 1), ('research interpretive enterprise', 1)]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "# set directory path\n",
    "dirName = '../datasets/';    \n",
    "\n",
    "# set file name\n",
    "fileName = 'TestData_small.txt'\n",
    "\n",
    "# open and read file\n",
    "text = read_file_for_text(dirName, fileName)\n",
    "\n",
    "#clean punctuations\n",
    "clean_text = clean_punctuations(text)\n",
    "\n",
    "#split into words\n",
    "tokens = tokenize(clean_text)\n",
    "\n",
    "# filter stop words\n",
    "filtered_tokens = filter_stopwords(tokens)\n",
    "\n",
    "# convert to lower case (normalize)\n",
    "tokens_lower = convert_to_lower_case(filtered_tokens)\n",
    "\n",
    "# print tokens\n",
    "print(\"tokens : \\n \\n\",tokens_lower)\n",
    "\n",
    "unigrams= getNGrams(tokens_lower,1)\n",
    "print(\"\\n \\n unigrams: \\n \\n\",unigrams[:20])\n",
    "\n",
    "bigrams = getNGrams(tokens_lower,2)\n",
    "print(\"\\n \\n bigrams: \\n \\n\",bigrams[:20])\n",
    "\n",
    "trigrams= getNGrams(tokens_lower,3)\n",
    "print(\"\\n \\n trigrams: \\n \\n\",trigrams[:20])\n",
    "\n",
    "unigram_counts = ngrams_counts(tokens_lower, 1)\n",
    "print(\" \\n \\n unigrams with frequency \\n \\n\",list(islice(unigram_counts.items(), 10)))\n",
    "\n",
    "bigram_counts = ngrams_counts(tokens_lower, 2)\n",
    "print(\" \\n \\n bigrams with frequency \\n \\n\",list(islice(bigram_counts.items(), 10)))\n",
    "\n",
    "trigram_counts = ngrams_counts(tokens_lower, 3)\n",
    "print(\" \\n \\n bigrams with frequency \\n \\n\",list(islice(trigram_counts.items(), 10)))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
