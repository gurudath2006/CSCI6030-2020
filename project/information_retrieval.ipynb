{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSTRACT \n",
      "In Westermanâs [12] disruptive article, âQuantitative research as ? \n",
      "an interpretive e\n",
      "clean_text: ABSTRACT \n",
      "In Westermanâs 12 disruptive article âQuantitative research as  \n",
      "an interpretive enterprise The mostly unacknowledged role of \n",
      "interpretation in research efforts and suggestions for explicitly \n",
      "interpretive quantitative investigationsâ he invited qualitative \n",
      "researchers in psychology to adopt quantitative methods into \n",
      "interpretive inquiry given that they were as capable as qualitative \n",
      "measures in producing meaningladen results The objective of \n",
      "this article is to identify Westermanâs 12 key arguments and \n",
      "apply them to the practice of Learning Analytics in educational \n",
      "interventions The primary implication for Learning Analytics \n",
      "practitioners is the need to interpret quantitative analysis \n",
      "procedures at every phase from philosophy to conclusions \n",
      "Furthermore Learning Analytics practitioners and consumers \n",
      "must critically examine any assumption that suggests quantitative \n",
      "methodologies in Learning Analytics are inherently objective or \n",
      "that Learning Analytics algorithms may replace judgment rather \n",
      "than aid it Lastly we propose a method for making observational \n",
      "data in virtual environments concrete through nested models   \n",
      "\n",
      "\n",
      "In traditional cognitive science inquiry measurement almost \n",
      "always involves significant levels of abstraction away from the \n",
      "phenomena of interest Usually observable behavior is of interest \n",
      "because it is assumed to indicate cognitive phenomena For \n",
      "example in learning measurement the factors of interest are \n",
      "tokens ['ABSTRACT', 'In', 'Westermanâ\\x80\\x99s', '12', 'disruptive', 'article', 'â\\x80\\x9cQuantitative', 'research', 'as', 'an', 'interpretive', 'enterprise', 'The', 'mostly', 'unacknowledged', 'role', 'of', 'interpretation', 'in', 'research', 'efforts', 'and', 'suggestions', 'for', 'explicitly', 'interpretive', 'quantitative', 'investigationsâ\\x80\\x9d', 'he', 'invited', 'qualitative', 'researchers', 'in', 'psychology', 'to', 'adopt', 'quantitative', 'methods', 'into', 'interpretive', 'inquiry', 'given', 'that', 'they', 'were', 'as', 'capable', 'as', 'qualitative', 'measures']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# load data\n",
    "filename = 'TestData_small.txt'\n",
    "file = open(filename, encoding=\"iso-8859-1\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "print(text[:100])\n",
    "\n",
    "#clean punctuations\n",
    "def clean_text(text):\n",
    "    clean_text= \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "clean_text= clean_text(text)\n",
    "print(\"clean_text:\",clean_text)\n",
    "\n",
    "#split into words(tokenize)\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(clean_text)\n",
    "print(\"tokens\",tokens[:50])\n",
    "\n",
    "\n",
    "# split based on words only\n",
    "#import re\n",
    "#words = re.split(r'\\W+', sentences)\n",
    "#print(\"words\",words[:100])\n",
    "\n",
    "\n",
    "\n",
    "# convert to lower case (normalize)\n",
    "#tokens_lower = [w.lower() for w in tokens_nltk]\n",
    "#print(\"tokens_nltk\",tokens_lower)\n",
    "\n",
    "\n",
    "# filter out stop words\n",
    "#from nltk.corpus import stopwords\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "#words = [w for w in words if not w in stop_words]\n",
    "#print(words[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach might be to use the regex model (re) and split the document into words by selecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and ‘_’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
